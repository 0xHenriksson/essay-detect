{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/0xHenriksson/essay-detect/blob/main/detect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZtZLAeBcWeL"
      },
      "source": [
        "# Essay Analyzer\n",
        "\n",
        "Objective: Determine if an essay submitted by a student is written by AI or not.\n",
        "\n",
        "Methods;\n",
        "- Using the original assignment instructions, prompt popular model providers (OpenAI, Anthropic, Google) for ~10 complete essays.\n",
        "- Create an embedding of the AI essays\n",
        "- Use FAISS and other analytical methods to compare the student's essay to the AI essays\n",
        "    - use either the embedding or the text directly\n",
        "\n",
        "## NLP Methods\n",
        "- semantic similarity\n",
        "- syntactical similarity\n",
        "## Statistical Methods\n",
        "- word and phrase frequency\n",
        "- perplexity\n",
        "- stylometry\n",
        "- semantic coherence\n",
        "\n",
        "## Mathematical Methods\n",
        "- Total Variation distance\n",
        "- Perturbation discrepancy detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_gJIOHY5tR4A",
        "outputId": "817281eb-4f28-4da2-ace2-6747af1e7894",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: SCIPY in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "# special installs\n",
        "!pip install transformers torch spacy nltk SCIPY faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OzYLUBbmtVYT",
        "outputId": "aaae7440-e862-46d1-c82b-b1c34311aa97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Download the spaCy language model\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Kp8DNgoobxcQ",
        "outputId": "c543c79b-4bb2-4c06-8bd0-d42ef51f2f01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'scipy.spatial.distances'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b62c7e8aea1b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistances\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy.spatial.distances'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from scipy.spatial.distances import cosine\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import torch\n",
        "from typing import List, Tuple, Dict\n",
        "import spacy\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOU6ctH1h-Pv"
      },
      "source": [
        "Load in the essay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "SHWPAYNeiOmN",
        "outputId": "3353485e-b08d-4b6f-a822-2a40b0795432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/detect/essay.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3b26e4f58060>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# read the essay text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/detect/essay.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0messay_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/detect/essay.txt'"
          ]
        }
      ],
      "source": [
        "# Google Drive Method\n",
        "#  mount the drive\n",
        "from google.colab import drive\n",
        "# authentication\n",
        "from google.colab import auth\n",
        "\n",
        "username = \"\"\n",
        "password = \"\"\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# read the essay text\n",
        "with open('/content/drive/MyDrive/detect/student_ essay.txt', 'r') as file:\n",
        "    essay_text = file.read()\n",
        "\n",
        "# read in the assignment text to be used for prompting\n",
        "with open('/content/drive/MyDrive/detect/assignment.txt', 'r') as file:\n",
        "  assignment_text = file.read()\n",
        "\n",
        "# read in all 10 ai generated essays labeled oai_essay{1-10}.txt\n",
        "with open('/content/drive/MyDrive/detect/oai_essay1.txt', 'r') as file:\n",
        "    oai_essay1 = file.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/detect/oai_essay2.txt', 'r') as file:\n",
        "    oai_essay2 = file.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/detect/oai_essay3.txt', 'r') as file:\n",
        "    oai_essay3 = file.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/detect/oai_essay4.txt', 'r') as file:\n",
        "    oai_essay4 = file.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/detect/oai_essay5.txt', 'r') as file:\n",
        "    oai_essay5 = file.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/detect/oai_essay6.txt', 'r') as file:\n",
        "    oai_essay6 = file.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/detect/oai_essay7.txt', 'r') as file:\n",
        "    oai_essay7 = file.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/detect/oai_essay8.txt', 'r') as file:\n",
        "    oai_essay8 = file.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/detect/oai_essay9.txt', 'r') as file:\n",
        "    oai_essay9 = file.read()\n",
        "\n",
        "with open('/content/drive/MyDrive/detect/oai_essay10.txt', 'r') as file:\n",
        "    oai_essay10 = file.read()\n",
        "\n",
        "# put them all oai_essay in a list\n",
        "essay_list = [oai_essay1, oai_essay2, oai_essay3, oai_essay4, oai_essay5, oai_essay6, oai_essay7, oai_essay8, oai_essay9, oai_essay10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s96HIOcLuL9p"
      },
      "source": [
        "# Prompt Construction\n",
        "Attempt to reconstruct a prompt similar to what the student may have used when asking the AI to write the essay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "_N4w3xNYuB9n",
        "outputId": "2f93cd54-9781-4ed3-e020-8fce23c12b58"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Write an essay about Hedda Gabler by Henrik Ibsen that follows these requirements Fiction Analysis Essay:  Theme\\nAP Literature and Composition  (100 Points)\\n\\nText Options:\\nNative Son by Richard Wright\\nWuthering Heights by Emily Bronte\\nFrankenstein by Mary Shelley\\n“Babylon Revisited” by F. Scott Fitzgerald\\n“Hunters in the Snow” by Tobias Wolff\\n\\nPrompt:\\nAuthors become famous for their ability to use complex literary devices to reveal a criticism, reflect a social or political issue, and/or discuss the complexity and truth of human nature into the meaning of their work as a whole.  Simply stated, to understand theme is to understand an author’s work.\\n\\nIn a fully-developed composition (no longer than four pages in MLA format), analyze several complex themes (approximately 3; better to go deep) in a work of your choice that we have read this semester. Make sure you take a unique approach while analyzing this piece, incorporate and properly use textual evidence, and fully support a clearly-defined original thesis.  Your essay should clearly reveal the writer’s purpose and overarching goals in writing this work. \\n\\nPlease note:  Themes stated as motifs will not be accepted and result in a failing grade (No credit example:  One theme in Wuthering Heights is all-consuming passion.)  I suggest you review your theme notes, half sheet, sample themes we have written in class, and chapter 4 in Perrine’s before you begin.  Part of this is proving you are competent at constructing proper theme statements.  When in doubt, have your teacher look over them.  \\n\\nSome questions to ask yourself:\\nWhat universal truths is the writer trying to portray to the reader?  What does the text make one ponder and perhaps think differently about after reading it?  \\nBy what means does this theme become revealed?  What were key plot events, characters, conflicts, aspects of narration, etc. that illustrated each theme? \\nHow can I best defend my theme statements/thesis?  What is the best evidence in the text to quote and paraphrase to support my ideas?\\nWhat outside research agrees with my interpretations?  Is it from a university or other academic organization worthy of a college paper? (No e-notes, Spark Notes, Cliff notes, AI, Wikipedia, LitCharts, etc.) \\nWhat can I include from Foster’s book that is related to my theme statements?  How does the genre, time period, author’s background, and other external factors influence the themes of this text as well?   \\n\\n\\nMisc. Requirements:\\n-Approx. 3-4 pages in length, double-spaced, 12 point, plain font in MLA format\\n-hard copy and electronic copy on Turnitin submitted on due date (day of midterm)\\n-separate Works Cited page with a minimum of four credible sources in MLA format:\\n\\t-Foster’s book, the piece itself, (2) outside research:  academic journals only\\n-proper in-text citations of the researched information and quotes from the literary work\\n-ABSOLUTELY NO PLAGIARISM or use of AI or you will fail this assignment and receive a discipline referral.\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# put the title of the media here (book, play, poem, etc)\n",
        "media_title = \"Hedda Gabler\"\n",
        "author = \"Henrik Ibsen\"\n",
        "\n",
        "prompt = f\"Write an essay about {media_title} by {author} that follows these requirements {assignment_text}\"\n",
        "\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jgPbVFcYV6Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiHvSsDDfFS2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import spacy\n",
        "from typing import List, Tuple, Dict\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "@dataclass\n",
        "class TextEmbedding:\n",
        "    text: str\n",
        "    embedding: np.ndarray\n",
        "    metadata: Dict = None\n",
        "\n",
        "class FAISSTextAnalyzer:\n",
        "    def __init__(self, model_name: str = \"sentence-transformers/all-mpnet-base-v2\"):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "        # Initialize FAISS index\n",
        "        self.embedding_dim = self.model.config.hidden_size\n",
        "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
        "\n",
        "        # Enable GPU if available\n",
        "        if torch.cuda.is_available():\n",
        "            self.res = faiss.StandardGpuResources()\n",
        "            self.index = faiss.index_cpu_to_gpu(self.res, 0, self.index)\n",
        "\n",
        "        # Store text embeddings for retrieval\n",
        "        self.stored_embeddings: List[TextEmbedding] = []\n",
        "\n",
        "    def get_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Generate embedding for a single text.\"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        return embedding.cpu().numpy()\n",
        "\n",
        "    def add_texts(self, texts: List[str], metadata: List[Dict] = None):\n",
        "        \"\"\"Add multiple texts to the FAISS index.\"\"\"\n",
        "        if metadata is None:\n",
        "            metadata = [None] * len(texts)\n",
        "\n",
        "        embeddings = []\n",
        "        for text, meta in zip(texts, metadata):\n",
        "            embedding = self.get_embedding(text)\n",
        "            self.stored_embeddings.append(TextEmbedding(text=text, embedding=embedding, metadata=meta))\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        embeddings = np.vstack(embeddings)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "    def search(self, query_text: str, k: int = 5) -> List[Tuple[float, TextEmbedding]]:\n",
        "        \"\"\"Search for similar texts and return distances and text objects.\"\"\"\n",
        "        query_emb = self.get_embedding(query_text)\n",
        "        D, I = self.index.search(query_emb, k)\n",
        "\n",
        "        results = []\n",
        "        for dist, idx in zip(D[0], I[0]):\n",
        "            results.append((dist, self.stored_embeddings[idx]))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def syntactic_features(self, text: str) -> Dict[str, float]:\n",
        "        \"\"\"Extract syntactic features using spaCy.\"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        features = defaultdict(int)\n",
        "\n",
        "        # Analyze sentence structure\n",
        "        for sent in doc.sents:\n",
        "            features['sent_length'] += len(sent)\n",
        "            features['dep_tree_depth'] += self._get_dep_tree_depth(sent.root)\n",
        "\n",
        "        # POS tag distribution\n",
        "        for token in doc:\n",
        "            features[f'pos_{token.pos_}'] += 1\n",
        "\n",
        "        # Normalize features\n",
        "        total_tokens = len(doc)\n",
        "        return {k: v/total_tokens for k, v in features.items()}\n",
        "\n",
        "    def _get_dep_tree_depth(self, root) -> int:\n",
        "        \"\"\"Calculate dependency tree depth recursively.\"\"\"\n",
        "        if not list(root.children):\n",
        "            return 0\n",
        "        return 1 + max(self._get_dep_tree_depth(child) for child in root.children)\n",
        "\n",
        "    def compare_texts(self, original: str, generated_samples: List[str]) -> Dict[str, List[float]]:\n",
        "        \"\"\"Compare original text against multiple generated samples using FAISS.\"\"\"\n",
        "        results = {\n",
        "            'semantic_similarity': [],\n",
        "            'syntactic_similarity': [],\n",
        "            'combined_score': []\n",
        "        }\n",
        "\n",
        "        # Get features for original text\n",
        "        orig_embedding = self.get_embedding(original)\n",
        "        orig_syntactic = self.syntactic_features(original)\n",
        "\n",
        "        # Add generated samples to FAISS index\n",
        "        self.index.reset()  # Clear existing index\n",
        "        sample_embeddings = []\n",
        "        for sample in generated_samples:\n",
        "            emb = self.get_embedding(sample)\n",
        "            sample_embeddings.append(emb)\n",
        "        sample_embeddings = np.vstack(sample_embeddings)\n",
        "        self.index.add(sample_embeddings)\n",
        "\n",
        "        # Get nearest neighbors for original text\n",
        "        D, I = self.index.search(orig_embedding, len(generated_samples))\n",
        "\n",
        "        for i, sample in enumerate(generated_samples):\n",
        "            # Semantic similarity from FAISS distance\n",
        "            semantic_sim = 1.0 / (1.0 + D[0][i])  # Convert distance to similarity score\n",
        "\n",
        "            # Syntactic similarity using feature vectors\n",
        "            sample_syntactic = self.syntactic_features(sample)\n",
        "            syntactic_sim = 1.0 - cosine(\n",
        "                np.array(list(orig_syntactic.values())),\n",
        "                np.array(list(sample_syntactic.values()))\n",
        "            )\n",
        "\n",
        "            # Combined score (weighted average)\n",
        "            combined = 0.6 * semantic_sim + 0.4 * syntactic_sim\n",
        "\n",
        "            results['semantic_similarity'].append(semantic_sim)\n",
        "            results['syntactic_similarity'].append(syntactic_sim)\n",
        "            results['combined_score'].append(combined)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def batch_compare(self, texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
        "        \"\"\"Efficiently compare multiple texts in batches.\"\"\"\n",
        "        n = len(texts)\n",
        "        similarity_matrix = np.zeros((n, n))\n",
        "\n",
        "        # Process in batches\n",
        "        for i in range(0, n, batch_size):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            batch_embeddings = []\n",
        "\n",
        "            for text in batch_texts:\n",
        "                emb = self.get_embedding(text)\n",
        "                batch_embeddings.append(emb)\n",
        "\n",
        "            batch_embeddings = np.vstack(batch_embeddings)\n",
        "\n",
        "            # Compare batch against all texts\n",
        "            D, _ = self.index.search(batch_embeddings, n)\n",
        "            similarity_matrix[i:i + len(batch_texts)] = 1.0 / (1.0 + D)\n",
        "\n",
        "        return similarity_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCGe2f8u8fmL"
      },
      "outputs": [],
      "source": [
        "# create an embedding of the model outputs\n",
        "# Initialize FAISS analyzer\n",
        "faiss_analyzer = FAISSTextAnalyzer()\n",
        "\n",
        "# Create embeddings for all generated essays\n",
        "embeddings = []\n",
        "for essay in essay_list:\n",
        "    embedding = faiss_analyzer.get_embedding(essay)\n",
        "    embeddings.append(embedding)\n",
        "\n",
        "# Stack embeddings into matrix\n",
        "essay_embeddings = np.vstack(embeddings)\n",
        "\n",
        "# Add essays to FAISS index for similarity search\n",
        "faiss_analyzer.index.reset()\n",
        "faiss_analyzer.add_texts(essay_list)\n",
        "\n",
        "# Get similarity matrix for all essays\n",
        "similarity_matrix = faiss_analyzer.batch_compare(essay_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox_ev0_q8mjr"
      },
      "outputs": [],
      "source": [
        "# plot the embedding\n",
        "# Reduce dimensionality to 3D using PCA\n",
        "pca = PCA(n_components=3)\n",
        "embeddings_3d = pca.fit_transform(essay_embeddings)\n",
        "\n",
        "# Create 3D scatter plot\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot points\n",
        "scatter = ax.scatter(embeddings_3d[:, 0],\n",
        "                    embeddings_3d[:, 1],\n",
        "                    embeddings_3d[:, 2],\n",
        "                    c=range(len(embeddings_3d)), # color by index\n",
        "                    cmap='viridis')\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('PC1')\n",
        "ax.set_ylabel('PC2')\n",
        "ax.set_zlabel('PC3')\n",
        "ax.set_title('3D PCA of Essay Embeddings')\n",
        "\n",
        "# Add colorbar\n",
        "plt.colorbar(scatter, label='Essay Index')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6o2z51LVJRF"
      },
      "source": [
        "# Semantic Similarity Analysis\n",
        "\n",
        "Semantic similarity measures how close two texts are in terms of their meaning and content by comparing their vector embeddings in high-dimensional space. Key aspects:\n",
        "\n",
        "- Uses neural language models to create dense vector representations\n",
        "- Captures deeper meaning beyond surface-level word matching\n",
        "- Accounts for synonyms, paraphrasing, and concept relationships\n",
        "\n",
        "This helps detect AI-generated essays because:\n",
        "1. AI models often reuse similar patterns and phrasings from training data\n",
        "2. Multiple AI-generated essays on the same topic may cluster together\n",
        "3. Human writing shows more semantic diversity and originality\n",
        "\n",
        "Interpreting the scores:\n",
        "- Scores range from 0 (completely different) to 1 (identical meaning)\n",
        "- Scores > 0.85 suggest suspiciously similar content\n",
        "- Natural human writing typically shows scores of 0.3-0.7 when compared\n",
        "- Look for clusters of essays with unusually high mutual similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-1dVmqA8wp1"
      },
      "outputs": [],
      "source": [
        "# compare semantic similarity\n",
        "# Get semantic similarity scores between essays using FAISS analyzer\n",
        "semantic_scores = []\n",
        "for i, essay in enumerate(essay_list[1:]):  # Compare against first essay\n",
        "    results = faiss_analyzer.compare_texts(essay_list[0], [essay])\n",
        "    semantic_scores.append(results['semantic_similarity'][0])\n",
        "\n",
        "print(f\"Semantic similarity scores (compared to original essay):\")\n",
        "for i, score in enumerate(semantic_scores):\n",
        "    print(f\"Essay {i+1}: {score:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXqEwcE2VJRG"
      },
      "source": [
        "# Syntactical Similarity Analysis\n",
        "\n",
        "Syntactical similarity measures how similar two texts are in terms of their grammatical structure and language patterns, including:\n",
        "\n",
        "- Part-of-speech tag distributions\n",
        "- Dependency tree depths and patterns\n",
        "- Sentence structure complexity\n",
        "- Word order and transitions\n",
        "\n",
        "This is useful for AI detection because:\n",
        "1. AI models often generate text with more consistent/predictable syntactic patterns\n",
        "2. Human writing tends to have more natural variation in sentence structure\n",
        "3. Very high syntactic similarity between essays may indicate AI generation\n",
        "\n",
        "Interpreting the scores:\n",
        "- Scores range from 0 (completely different) to 1 (identical)\n",
        "- Scores > 0.8 suggest suspiciously similar syntax patterns\n",
        "- Natural human writing typically shows scores of 0.4-0.7 when compared\n",
        "- Look for outliers with unusually high similarity to other essays\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AukKMlbr-bAU"
      },
      "outputs": [],
      "source": [
        "# compare syntactical similarity\n",
        "# Get syntactic similarity scores between essays using FAISS analyzer\n",
        "syntactic_scores = []\n",
        "for i, essay in enumerate(essay_list[1:]):  # Compare against first essay\n",
        "    results = faiss_analyzer.compare_texts(essay_list[0], [essay])\n",
        "    syntactic_scores.append(results['syntactic_similarity'][0])\n",
        "\n",
        "print(f\"Syntactic similarity scores (compared to original essay):\")\n",
        "for i, score in enumerate(syntactic_scores):\n",
        "    print(f\"Essay {i+1}: {score:.3f}\")\n",
        "\n",
        "# Plot semantic vs syntactic similarity\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(semantic_scores, syntactic_scores)\n",
        "plt.xlabel('Semantic Similarity')\n",
        "plt.ylabel('Syntactic Similarity')\n",
        "plt.title('Semantic vs Syntactic Similarity')\n",
        "\n",
        "# Add essay labels\n",
        "for i in range(len(semantic_scores)):\n",
        "    plt.annotate(f'Essay {i+1}', (semantic_scores[i], syntactic_scores[i]))\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Klt-aHV_Kkf"
      },
      "source": [
        "# Statistical Methods\n",
        "\n",
        "### Word and Phrase Frequency Analysis\n",
        "\n",
        "Word and phrase frequency analysis examines how often specific words and combinations of words appear in text.\n",
        "This can help detect AI-generated content because:\n",
        "\n",
        "1. AI models often show distinctive patterns in word usage and repetition\n",
        "2. Humans tend to have more natural variation in word choice\n",
        "3. Certain phrases or n-grams may appear more frequently in AI text\n",
        "\n",
        "N-grams are contiguous sequences of n items (words/tokens) from a text:\n",
        "- Unigrams (n=1): Single words like \"the\", \"cat\", \"sat\"\n",
        "- Bigrams (n=2): Pairs of words like \"the cat\", \"cat sat\"\n",
        "- Trigrams (n=3): Three word sequences like \"the cat sat\"\n",
        "\n",
        "N-grams help analyze text patterns and transitions between words. AI models may\n",
        "show distinctive n-gram patterns compared to human writing.\n",
        "\n",
        "\n",
        "The analysis below looks at:\n",
        "- Individual word frequencies: Shows if vocabulary usage is too repetitive or unnatural\n",
        "- Bigram overlap: High overlap (>0.8) between essays suggests potential AI generation\n",
        "- Jensen-Shannon distance: Measures how similar the word distributions are\n",
        "  - Values closer to 0 indicate very similar distributions (potential AI)\n",
        "  - Values closer to 1 indicate more natural human variation\n",
        "\n",
        "When interpreting results, look for:\n",
        "- Unnaturally high similarity in word frequencies between essays\n",
        "- Repetitive phrases or formulaic language patterns\n",
        "- Word distribution patterns that are too \"clean\" or systematic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQY7jBhA-zJg"
      },
      "outputs": [],
      "source": [
        "# calculate word and phrase frequency, compare to model outputs\n",
        "# Calculate word and phrase frequencies\n",
        "def get_freq_stats(text):\n",
        "    # Tokenize and get word frequencies\n",
        "    words = word_tokenize(text.lower())\n",
        "    word_freq = Counter(words)\n",
        "\n",
        "    # Get bigram frequencies\n",
        "    bi_grams = list(ngrams(words, 2))\n",
        "    bigram_freq = Counter(bi_grams)\n",
        "\n",
        "    # Get trigram frequencies\n",
        "    tri_grams = list(ngrams(words, 3))\n",
        "    trigram_freq = Counter(tri_grams)\n",
        "\n",
        "    return word_freq, bigram_freq, trigram_freq\n",
        "\n",
        "# Get frequencies for student essay\n",
        "student_freqs = get_freq_stats(essay_list[0])\n",
        "\n",
        "# Get frequencies for AI essays and calculate differences\n",
        "for i, essay in enumerate(essay_list[1:]):\n",
        "    ai_freqs = get_freq_stats(essay)\n",
        "\n",
        "    # Compare top words\n",
        "    print(f\"\\nEssay {i+1} frequency comparison:\")\n",
        "    print(\"\\nTop 10 words comparison:\")\n",
        "    student_top = dict(sorted(student_freqs[0].items(), key=lambda x:x[1], reverse=True)[:10])\n",
        "    ai_top = dict(sorted(ai_freqs[0].items(), key=lambda x:x[1], reverse=True)[:10])\n",
        "    print(f\"Student essay: {student_top}\")\n",
        "    print(f\"AI essay: {ai_top}\")\n",
        "\n",
        "    # Compare bigram overlap\n",
        "    student_bigrams = set(student_freqs[1].keys())\n",
        "    ai_bigrams = set(ai_freqs[1].keys())\n",
        "    bigram_overlap = len(student_bigrams.intersection(ai_bigrams)) / len(student_bigrams)\n",
        "    print(f\"\\nBigram overlap ratio: {bigram_overlap:.3f}\")\n",
        "\n",
        "    # Compare distribution similarity using Jensen-Shannon distance\n",
        "    all_words = set(student_freqs[0].keys()).union(set(ai_freqs[0].keys()))\n",
        "    p = np.array([student_freqs[0].get(w, 0) for w in all_words])\n",
        "    q = np.array([ai_freqs[0].get(w, 0) for w in all_words])\n",
        "    p = p / p.sum()  # normalize\n",
        "    q = q / q.sum()\n",
        "    m = 0.5 * (p + q)\n",
        "    js_dist = 0.5 * (entropy(p, m) + entropy(q, m))\n",
        "    print(f\"Jensen-Shannon distance: {js_dist:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4du15NMMVJRH"
      },
      "source": [
        "### Perplexity Analysis\n",
        "Perplexity measures how \"surprised\" a language model is by a text. Lower perplexity means the text follows patterns the model expects.\n",
        "\n",
        "For AI detection:\n",
        "- AI text tends to have lower perplexity (more predictable patterns) than human text\n",
        "- Human writing is typically more \"surprising\" with higher perplexity scores\n",
        "- Large perplexity differences between essays suggest different authors/sources\n",
        "\n",
        "Interpreting the scores:\n",
        "- Student essay perplexity serves as baseline\n",
        "- AI essays with perplexity >20% lower than student = likely AI generated\n",
        "- Similar perplexity scores (±20%) = inconclusive\n",
        "- Higher perplexity in AI essays = possible human writing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhIKKV3v-42E"
      },
      "outputs": [],
      "source": [
        "# calculate essay perplexity and compare to model output perplexity\n",
        "def calculate_perplexity(text, model, tokenizer):\n",
        "    # encode text\n",
        "    encodings = tokenizer(text, return_tensors='pt').to(model.device)\n",
        "    max_length = 1024  # prevent OOM on long texts\n",
        "\n",
        "    # handle long sequences by splitting into chunks\n",
        "    stride = 512\n",
        "    nlls = []\n",
        "    for i in range(0, encodings.input_ids.size(1), stride):\n",
        "        begin = max(i - stride, 0)\n",
        "        end = min(i + max_length, encodings.input_ids.size(1))\n",
        "        trg_len = end - i\n",
        "\n",
        "        input_ids = encodings.input_ids[:, begin:end]\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100 # ignore non-targets\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, labels=target_ids)\n",
        "            neg_log_likelihood = outputs.loss\n",
        "\n",
        "        nlls.append(neg_log_likelihood)\n",
        "\n",
        "    return torch.exp(torch.stack(nlls).mean())\n",
        "\n",
        "# calculate perplexity for student essay\n",
        "student_perplexity = calculate_perplexity(essay_list[0], gpt2_model, gpt2_tokenizer)\n",
        "print(f\"\\nStudent essay perplexity: {student_perplexity:.2f}\")\n",
        "\n",
        "# calculate and compare perplexity for AI essays\n",
        "for i, essay in enumerate(essay_list[1:]):\n",
        "    ai_perplexity = calculate_perplexity(essay, gpt2_model, gpt2_tokenizer)\n",
        "    diff = ((ai_perplexity - student_perplexity) / student_perplexity) * 100\n",
        "    print(f\"AI essay {i+1} perplexity: {ai_perplexity:.2f} ({diff:+.1f}% vs student)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6gfVSyIVJRH"
      },
      "source": [
        "### Stylometry Comparison\n",
        "Stylometry is the statistical analysis of writing style. It examines measurable patterns in text like:\n",
        "- Average sentence/word length\n",
        "- Lexical density (ratio of content words to total words)\n",
        "- Function word usage (common words like \"the\", \"and\", \"of\")\n",
        "- Punctuation patterns\n",
        "\n",
        "We use these features to detect AI-generated text since language models often produce writing with:\n",
        "- More consistent/uniform sentence lengths\n",
        "- Higher lexical density (more varied vocabulary)\n",
        "- More regular punctuation patterns\n",
        "- Less natural function word usage\n",
        "\n",
        "Interpreting the scores:\n",
        "- Large differences (>15-20%) between human/AI essays in any metric are suspicious\n",
        "- AI text tends to be more \"regular\" - fewer stylistic variations\n",
        "- Human writing typically shows more natural variation in these metrics\n",
        "- Multiple metrics showing AI-like patterns increases confidence in detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFMHTeNX-_DG"
      },
      "outputs": [],
      "source": [
        "# compare stylometry\n",
        "\n",
        "# calculate stylometric features using spacy\n",
        "def get_stylometric_features(text, nlp):\n",
        "    doc = nlp(text)\n",
        "    features = {\n",
        "        'avg_sent_len': 0,\n",
        "        'avg_word_len': 0,\n",
        "        'lexical_density': 0,\n",
        "        'function_words': 0,\n",
        "        'punctuation_ratio': 0\n",
        "    }\n",
        "\n",
        "    # sentence and word length\n",
        "    sent_lengths = [len(sent) for sent in doc.sents]\n",
        "    word_lengths = [len(token.text) for token in doc if token.is_alpha]\n",
        "    features['avg_sent_len'] = np.mean(sent_lengths)\n",
        "    features['avg_word_len'] = np.mean(word_lengths)\n",
        "\n",
        "    # lexical density (content words / total words)\n",
        "    content_words = len([t for t in doc if not t.is_stop and t.is_alpha])\n",
        "    total_words = len([t for t in doc if t.is_alpha])\n",
        "    features['lexical_density'] = content_words / total_words if total_words > 0 else 0\n",
        "\n",
        "    # function word ratio\n",
        "    function_words = len([t for t in doc if t.is_stop])\n",
        "    features['function_words'] = function_words / total_words if total_words > 0 else 0\n",
        "\n",
        "    # punctuation ratio\n",
        "    punct = len([t for t in doc if t.is_punct])\n",
        "    features['punctuation_ratio'] = punct / len(doc) if len(doc) > 0 else 0\n",
        "\n",
        "    return features\n",
        "\n",
        "# get features for student essay\n",
        "student_style = get_stylometric_features(essay_list[0], nlp)\n",
        "print(\"\\nStudent essay stylometric features:\")\n",
        "for k,v in student_style.items():\n",
        "    print(f\"{k}: {v:.3f}\")\n",
        "\n",
        "# compare with AI essays\n",
        "for i, essay in enumerate(essay_list[1:]):\n",
        "    ai_style = get_stylometric_features(essay, nlp)\n",
        "    print(f\"\\nAI essay {i+1} stylometric differences:\")\n",
        "    for k in student_style:\n",
        "        diff = ((ai_style[k] - student_style[k]) / student_style[k]) * 100\n",
        "        print(f\"{k}: {diff:+.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmkIQANlVJRI"
      },
      "source": [
        "### Semantic Coherence Analysis\n",
        "\n",
        "Semantic coherence measures how well consecutive sentences flow and connect logically by comparing their semantic similarity.\n",
        "Higher coherence scores (closer to 1.0) indicate smoother transitions between ideas, while lower scores suggest more abrupt changes.\n",
        "\n",
        "AI models often produce text with unusually high coherence since they optimize for consistent semantic flow.\n",
        "Human writing typically shows more natural variation in coherence as writers balance flow with emphasis and style.\n",
        "\n",
        "Interpreting the results:\n",
        "- If AI essays show significantly higher coherence (+10-30%) vs student essay: Likely AI generated\n",
        "- If coherence differences are minimal (-10% to +10%): Inconclusive, need other metrics\n",
        "- If AI essays show lower coherence: Likely human written, or poorly generated AI text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUhSgci3_DlP"
      },
      "outputs": [],
      "source": [
        "# compute and compare semantic coherence\n",
        "def get_semantic_coherence(text, nlp):\n",
        "    # break into sentences and get embeddings\n",
        "    sents = [sent.text.strip() for sent in nlp(text).sents]\n",
        "    if len(sents) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    # get embeddings for each sentence\n",
        "    embeddings = []\n",
        "    for sent in sents:\n",
        "        doc = nlp(sent)\n",
        "        if doc.has_vector:\n",
        "            embeddings.append(doc.vector)\n",
        "\n",
        "    # calculate cosine similarity between consecutive sentences\n",
        "    coherence_scores = []\n",
        "    for i in range(len(embeddings)-1):\n",
        "        sim = 1 - cosine(embeddings[i], embeddings[i+1])\n",
        "        coherence_scores.append(sim)\n",
        "\n",
        "    return np.mean(coherence_scores)\n",
        "\n",
        "# get coherence scores\n",
        "student_coherence = get_semantic_coherence(essay_list[0], nlp)\n",
        "print(f\"\\nStudent essay semantic coherence: {student_coherence:.3f}\")\n",
        "\n",
        "ai_coherences = []\n",
        "for i, essay in enumerate(essay_list[1:]):\n",
        "    coh = get_semantic_coherence(essay, nlp)\n",
        "    ai_coherences.append(coh)\n",
        "    print(f\"AI essay {i+1} semantic coherence: {coh:.3f}\")\n",
        "\n",
        "# compare differences\n",
        "print(\"\\nCoherence differences vs student essay:\")\n",
        "for i, coh in enumerate(ai_coherences):\n",
        "    diff = ((coh - student_coherence) / student_coherence) * 100\n",
        "    print(f\"AI essay {i+1}: {diff:+.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6iSuKh0_M2U"
      },
      "source": [
        "## Mathematical Methods\n",
        "\n",
        "###Total Variation Distance Analysis\n",
        "\n",
        "Total Variation (TV) distance measures the maximum difference between probability distributions.\n",
        "For essay analysis, we:\n",
        "1. Convert each essay's word frequencies into probability distributions\n",
        "2. Calculate TV distance between student and AI essays (0-1 scale)\n",
        "3. Compare distances to detect AI generation\n",
        "\n",
        "Interpretation:\n",
        "- TV distance close to 0: Very similar word distributions\n",
        "- TV distance close to 1: Very different word distributions\n",
        "- Higher TV distances suggest AI generation\n",
        "- Compare against mean TV distance to identify outliers\n",
        "\n",
        "Note: This complements other metrics since AI models may produce\n",
        "different word frequency patterns than human writers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8sgW1dt_Ibo",
        "outputId": "38701908-6765-4c7a-97b4-ee0862eb368f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'essay_list' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [counts[w] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(words)]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# calculate TV distances\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m student_dist \u001b[38;5;241m=\u001b[39m get_word_dist(essay_list[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     16\u001b[0m tv_distances \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, essay \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(essay_list[\u001b[38;5;241m1\u001b[39m:]):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'essay_list' is not defined"
          ]
        }
      ],
      "source": [
        "# calculate Total Variation distance between the essay and the model outputs\n",
        "def get_tv_distance(p, q):\n",
        "    # normalize to probability distributions\n",
        "    p = np.array(p) / np.sum(p)\n",
        "    q = np.array(q) / np.sum(q)\n",
        "    return 0.5 * np.sum(np.abs(p - q))\n",
        "\n",
        "# get word frequency distributions\n",
        "def get_word_dist(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    counts = Counter(words)\n",
        "    return [counts[w] for w in set(words)]\n",
        "\n",
        "# calculate TV distances\n",
        "student_dist = get_word_dist(essay_list[0])\n",
        "tv_distances = []\n",
        "\n",
        "for i, essay in enumerate(essay_list[1:]):\n",
        "    ai_dist = get_word_dist(essay)\n",
        "    tv_dist = get_tv_distance(student_dist, ai_dist)\n",
        "    tv_distances.append(tv_dist)\n",
        "    print(f\"TV distance between student and AI essay {i+1}: {tv_dist:.3f}\")\n",
        "\n",
        "# compare differences\n",
        "print(\"\\nTV distance differences vs mean:\")\n",
        "mean_tv = np.mean(tv_distances)\n",
        "for i, dist in enumerate(tv_distances):\n",
        "    diff = ((dist - mean_tv) / mean_tv) * 100\n",
        "    print(f\"AI essay {i+1}: {diff:+.1f}% from mean\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNZr2d9zVJRJ"
      },
      "source": [
        "### Perturbation Discrepancy Detection Method\n",
        "Perturbation Discrepancy Detection Method\n",
        "\n",
        "This method analyzes how stable/consistent the text embeddings are under small random perturbations.\n",
        "The intuition is that AI-generated text may show different patterns of embedding stability\n",
        "compared to human-written text.\n",
        "\n",
        "How it works:\n",
        "1. Get base embedding vector for the text\n",
        "2. Add small random noise multiple times to create perturbed versions\n",
        "3. Calculate mean cosine distance between the perturbed embeddings\n",
        "\n",
        "Interpretation:\n",
        "- Lower discrepancy scores indicate more stable/consistent embeddings\n",
        "- Higher discrepancy suggests more sensitivity to perturbations\n",
        "- AI text often shows different discrepancy patterns vs human text\n",
        "- Compare % differences between student and AI essays\n",
        "- Large deviations from student essay (>20%) may indicate AI generation\n",
        "\n",
        "Interpreting the scores:\n",
        "- Student essays typically show discrepancy scores around 0.1-0.3\n",
        "- AI essays often have scores 20-50% higher than human essays\n",
        "- If an essay's discrepancy is >20% higher than the student's score,\n",
        "  it suggests possible AI generation\n",
        "- Very low discrepancy (<0.1) can also be suspicious as it may indicate\n",
        "  unnaturally consistent text\n",
        "- Compare the % differences - larger gaps between student and AI scores\n",
        "  increase confidence in detection\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKVqKpYE_VWH"
      },
      "outputs": [],
      "source": [
        "# Perturbation discrepancy detection method\n",
        "# calculate perturbation discrepancy by adding small random noise to embeddings\n",
        "def get_perturb_discrepancy(text, n_perturb=10, noise_scale=0.01):\n",
        "    # get base embedding\n",
        "    base_emb = detector.faiss_analyzer.get_embedding(text)\n",
        "\n",
        "    # generate perturbed embeddings\n",
        "    perturbed = []\n",
        "    for _ in range(n_perturb):\n",
        "        noise = np.random.normal(0, noise_scale, base_emb.shape)\n",
        "        perturbed.append(base_emb + noise)\n",
        "    perturbed = np.vstack(perturbed)\n",
        "\n",
        "    # calculate mean cosine distance between perturbed versions\n",
        "    dists = []\n",
        "    for i in range(n_perturb):\n",
        "        for j in range(i+1, n_perturb):\n",
        "            dist = cosine(perturbed[i], perturbed[j])\n",
        "            dists.append(dist)\n",
        "    return np.mean(dists)\n",
        "\n",
        "# calculate discrepancy for student and AI essays\n",
        "student_disc = get_perturb_discrepancy(essay_list[0])\n",
        "ai_discs = []\n",
        "\n",
        "for i, essay in enumerate(essay_list[1:]):\n",
        "    disc = get_perturb_discrepancy(essay)\n",
        "    ai_discs.append(disc)\n",
        "    print(f\"Perturbation discrepancy for AI essay {i+1}: {disc:.3f}\")\n",
        "\n",
        "print(f\"\\nStudent essay discrepancy: {student_disc:.3f}\")\n",
        "\n",
        "# compare differences vs student\n",
        "print(\"\\nDiscrepancy differences vs student essay:\")\n",
        "for i, disc in enumerate(ai_discs):\n",
        "    diff = ((disc - student_disc) / student_disc) * 100\n",
        "    print(f\"AI essay {i+1}: {diff:+.1f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH0DpcjiVJRJ"
      },
      "source": [
        "## Ensemble Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSSAA3rjVJRK"
      },
      "outputs": [],
      "source": [
        "# ... existing code ...\n",
        "\n",
        "# Assuming you have run all the analysis cells and have the results stored, e.g.:\n",
        "# semantic_scores (compared to original essay)\n",
        "# syntactic_scores (compared to original essay)\n",
        "# js_distances (Jensen-Shannon distances)\n",
        "# student_perplexities, ai_perplexities\n",
        "# student_style, ai_styles\n",
        "# student_coherence, ai_coherences\n",
        "# tv_distances\n",
        "# student_disc, ai_discs\n",
        "\n",
        "def ensemble_analysis(student_essay, ai_essays, faiss_analyzer, nlp, gpt2_model, gpt2_tokenizer):\n",
        "    \"\"\"\n",
        "    Combines multiple analysis techniques to predict if essays are AI-generated.\n",
        "    \"\"\"\n",
        "    ensemble_results = []\n",
        "\n",
        "    # 1. Run individual analyses on all essays\n",
        "    all_essays = [student_essay] + ai_essays\n",
        "\n",
        "    semantic_similarities = faiss_analyzer.compare_texts(\n",
        "        student_essay, ai_essays)['semantic_similarity']\n",
        "    syntactic_similarities = faiss_analyzer.compare_texts(\n",
        "        student_essay, ai_essays)['syntactic_similarity']\n",
        "\n",
        "    js_distances = []\n",
        "    student_freqs = get_freq_stats(student_essay)\n",
        "    for essay in ai_essays:\n",
        "        ai_freqs = get_freq_stats(essay)\n",
        "        all_words = set(student_freqs[0].keys()).union(set(ai_freqs[0].keys()))\n",
        "        p = np.array([student_freqs[0].get(w, 0) for w in all_words])\n",
        "        q = np.array([ai_freqs[0].get(w, 0) for w in all_words])\n",
        "        p = p / p.sum() if p.sum() > 0 else p\n",
        "        q = q / q.sum() if q.sum() > 0 else q\n",
        "        m = 0.5 * (p + q)\n",
        "        js_dist = 0.5 * (entropy(p, m) + entropy(q, m)) if (p.sum() >\n",
        "                                                            0 and q.sum() > 0) else 1.0  # Handle cases with empty vocab\n",
        "        js_distances.append(js_dist)\n",
        "\n",
        "    ai_perplexities = [calculate_perplexity(\n",
        "        essay, gpt2_model, gpt2_tokenizer).item() for essay in ai_essays]\n",
        "    student_perplexity = calculate_perplexity(\n",
        "        student_essay, gpt2_model, gpt2_tokenizer).item()\n",
        "\n",
        "    ai_styles = [get_stylometric_features(essay, nlp) for essay in ai_essays]\n",
        "    student_style = get_stylometric_features(student_essay, nlp)\n",
        "\n",
        "    ai_coherences = [get_semantic_coherence(essay, nlp) for essay in ai_essays]\n",
        "    student_coherence = get_semantic_coherence(student_essay, nlp)\n",
        "\n",
        "    tv_distances = []\n",
        "    student_dist = get_word_dist(student_essay)\n",
        "    for essay in ai_essays:\n",
        "        ai_dist = get_word_dist(essay)\n",
        "        tv_dist = get_tv_distance(student_dist, ai_dist)\n",
        "        tv_distances.append(tv_dist)\n",
        "\n",
        "    ai_discs = [get_perturb_discrepancy(\n",
        "        essay, faiss_analyzer=faiss_analyzer) for essay in ai_essays]\n",
        "    student_disc = get_perturb_discrepancy(\n",
        "        student_essay, faiss_analyzer=faiss_analyzer)\n",
        "\n",
        "    # 2. Combine the scores with weights (you'll need to tune these weights)\n",
        "    for i, ai_essay in enumerate(ai_essays):\n",
        "        weights = {\n",
        "            \"semantic_similarity\": -0.2,  # Lower is more likely AI\n",
        "            \"syntactic_similarity\": -0.15,  # Lower is more likely AI\n",
        "            \"js_distance\": 0.1,       # Higher is more likely AI\n",
        "            \"perplexity_diff\": 0.1,   # Higher AI perplexity vs student is more likely human\n",
        "            \"style_diff\": 0.05,       # Significant style differences might indicate AI\n",
        "            \"coherence_diff\": -0.1,    # Higher coherence is more likely AI\n",
        "            \"tv_distance\": 0.15,      # Higher TV distance is more likely AI\n",
        "            \"perturb_discrepancy_diff\": -0.1  # Higher discrepancy is more likely AI\n",
        "        }\n",
        "\n",
        "        # Calculate weighted scores (you might need to normalize or adjust these based on your data)\n",
        "        weighted_score = 0\n",
        "\n",
        "        # Semantic Similarity (higher score = more similar = more likely AI if comparing AI-generated essays)\n",
        "        weighted_score += weights[\"semantic_similarity\"] * \\\n",
        "            semantic_similarities[i]\n",
        "\n",
        "        # Syntactic Similarity (higher score = more similar = more likely AI if comparing AI-generated essays)\n",
        "        weighted_score += weights[\"syntactic_similarity\"] * \\\n",
        "            syntactic_similarities[i]\n",
        "\n",
        "        # Jensen-Shannon Distance (lower score = more similar word distribution = more likely AI)\n",
        "        weighted_score += weights[\"js_distance\"] * js_distances[i]\n",
        "\n",
        "        # Perplexity Difference (lower perplexity for AI)\n",
        "        perplexity_diff = (ai_perplexities[i] - student_perplexity) / \\\n",
        "            student_perplexity if student_perplexity != 0 else 0\n",
        "        weighted_score += weights[\"perplexity_diff\"] * perplexity_diff\n",
        "\n",
        "        # Stylometry Differences (you might want to look at specific features here)\n",
        "        style_diff_sum = 0\n",
        "        for key in student_style:\n",
        "            diff = (ai_styles[i][key] - student_style[key]) / \\\n",
        "                student_style[key] if student_style[key] != 0 else 0\n",
        "            style_diff_sum += diff\n",
        "        weighted_score += weights[\"style_diff\"] * style_diff_sum\n",
        "\n",
        "        # Semantic Coherence Difference (higher coherence for AI)\n",
        "        coherence_diff = (ai_coherences[i] - student_coherence) / \\\n",
        "            student_coherence if student_coherence != 0 else 0\n",
        "        weighted_score += weights[\"coherence_diff\"] * coherence_diff\n",
        "\n",
        "        # Total Variation Distance (higher distance for AI)\n",
        "        weighted_score += weights[\"tv_distance\"] * tv_distances[i]\n",
        "\n",
        "        # Perturbation Discrepancy Difference (higher discrepancy for AI)\n",
        "        perturb_discrepancy_diff = (\n",
        "            ai_discs[i] - student_disc) / student_disc if student_disc != 0 else 0\n",
        "        weighted_score += weights[\"perturb_discrepancy_diff\"] * \\\n",
        "            perturb_discrepancy_diff\n",
        "\n",
        "        ensemble_results.append({\n",
        "            \"essay_index\": i + 1,\n",
        "            \"weighted_score\": weighted_score\n",
        "        })\n",
        "\n",
        "    return ensemble_results\n",
        "\n",
        "\n",
        "# Run the ensemble analysis\n",
        "ensemble_output = ensemble_analysis(\n",
        "    essay_list[0], essay_list[1:], faiss_analyzer, nlp, gpt2_model, gpt2_tokenizer\n",
        ")\n",
        "\n",
        "# 3. Interpret the results based on a threshold\n",
        "print(\"\\nEnsemble Analysis Results:\")\n",
        "threshold = 0.0  #tune this threshold\n",
        "for result in ensemble_output:\n",
        "    is_ai_generated = result[\"weighted_score\"] > threshold\n",
        "    print(f\"Essay {result['essay_index']}: Weighted Score = {result['weighted_score']:.3f} - {'AI Generated' if is_ai_generated else 'Likely Human'}\")\n",
        "\n",
        "# 4. Plot a histogram of the weighted scores\n",
        "plt.hist(\n",
        "    [result[\"weighted_score\"] for result in ensemble_output],\n",
        "    bins=20,\n",
        "    edgecolor=\"black\",\n",
        "    alpha=0.7,\n",
        ")\n",
        "plt.xlabel(\"Weighted Score\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Histogram of Weighted Scores\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}